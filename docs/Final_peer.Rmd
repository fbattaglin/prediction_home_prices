---
title: "Predict House Prices - Iowa"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

My task is to develop a model to predict the selling price of a given home in Ames, Iowa. We hope to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(ggplot2)
library(gridExtra)
library(MASS)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *
### Cleaning and Preprossesing the Data

Before start working on the model and EDA, it is important to summarise the data in order to see if there is any transformations that would be necessary to get the most of the data, and avoid any errors.

```{r}
summary(ames_train)
```

Based on the result of the summary, there is a few points we should address: 

1) First we should filter the dataset to contain only the Sale conditions that were **normal**, as the houses with non-normal selling conditions exhibit atypical behavior such as trade, foreclosure,sale between family members and others. Since we are interested in predict the price in normal conditions, this cases may disproportionately influence the model. 

2) Some categorical variables such as Fence, Garage.Qual and Garage.Cond have NA’s corresponding not to missing data but to another category such as “Not having a fence”/“Not having a garage”. Therefore we will transform those NA’s in a new category otherwise we may risk to incur a bias in the data and the modelling by discarding so many rows of data..

3) Some information treated as numerical (ex: MS.SubClass ) should actually represent categorical variables.

4) Last, we will create a new feature called *Age* that will represent the difference between the year the house was built and the year of our analysis (2018).


```{r}
ames_train <- ames_train %>%
  # Normal sale only
  filter(Sale.Condition == 'Normal') %>%
  # Transforming NAs
  mutate(Alley = if_else(is.na(Alley), 'No Alley Access', as.character(Alley)),
         Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
         Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
         Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
         BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
         BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
         Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
         Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
         Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
         Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
         Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
         Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
         Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
         Misc.Feature = if_else(is.na(Misc.Feature), 'No Misc Features', as.character(Misc.Feature)),
         # changing numerical variables to factor
         MS.SubClass = as.factor(MS.SubClass),
         Overall.Qual = as.factor(Overall.Qual),
         Overall.Cond = as.factor(Overall.Cond),
         # Creating Age Feature
         Age = as.numeric(2018 - Year.Built))
  
  
```


### EDA 

Now we need to understand how are the prices of the houses distributed in Ames, Iowa, we can best visualize this plotting an histogram.

```{r hist}
ggplot(ames_train, aes(price)) +
  geom_histogram(bins = 30, color= "black", fill= "#ffe680") +
  geom_vline(aes(xintercept = mean(price),col='mean'))+
  geom_vline(aes(xintercept = median(price),col='median')) +
  scale_color_manual(name = "Statistics", values = c(mean = "#ff4000", median = "#0066ff")) +
  xlab("Price USD")

summary(ames_train$price)
```

As we can see the median price is USD 155,500. The least expensive house is USD 39,300 and it is located in the neighborhood of BrkSide. On the other hand the most expensive house in Ames, Iowa is located in *NridgHt*, and it costs USD 615,000.


```{r boxplot}
ggplot(ames_train, aes(x=Neighborhood, y=price, fill=Neighborhood)) +
  geom_boxplot(alpha=0.3) +
  theme(legend.position="none", axis.text.x = element_text(angle = 90)) +
  geom_hline(yintercept =median(ames_train$price), col = "royalblue",lwd = 0.5)

ames_train %>% 
  group_by(Neighborhood) %>%
  summarise(Min=min(price, na.rm=TRUE),
            Max=max(price, na.rm=TRUE),
            Median=median(price, na.rm=TRUE),
            Mean=mean(price, na.rm=TRUE),
            IQR=IQR(price, na.rm=TRUE),
            Range= Max-Min) %>%
  arrange(desc(Median))

```

The mantra in real estate is "Location, Location, Location!".  The boxplot shows a large variety of distributions. Visually we can see that some of the most expensive neighborhoods (NPkVill, StoneBr, and Timber) have also the largest IQR. On the other hand, some of the neighborhoods with the lowest medians have also the most homogeneous prices.


```{r}
p1 <- ggplot(data = ames_train, aes(x=log(area), y=log(price)))+geom_point(colour="blue")+geom_smooth(method = "lm", col="red") + ggtitle("log(area) vs log(price)")

p2 <- ggplot(data = ames_train, aes(x=log(Lot.Area), y=log(price)))+geom_point(colour="green")+ geom_smooth(method = "lm", col="red") + ggtitle("log(Lot.Area) vs log(price)")

p3 <- ggplot(data = ames_train, aes(x=log(Lot.Frontage), y=log(price)))+geom_point(colour="purple")+ geom_smooth(method = "lm", col="red") + ggtitle("log(Lot.Frontage) vs log(price)")

p4 <- ggplot(data = ames_train, aes(x=Age, y=log(price)))+geom_point(colour="orange")+ geom_smooth(method = "lm", col="red") + ggtitle("Age vs log(price)")

grid.arrange(p1, p2, p3, p4, ncol=2, nrow=2)

```

Above we can se the relationship between price and other variables that are normally associated with the price of housing such as area and age of the house. All those feautures seem to be correlated to price either in a positive or negative manner (Age). 


```{r}
#Boxplot Overall Quality
b1 <- ggplot(ames_train, aes(x=Overall.Qual, y=price, fill=Overall.Qual)) +
  geom_boxplot(alpha=0.4) + theme(legend.position="none")

#Boxplot Overall Condition
b2 <- ggplot(ames_train, aes(x=Overall.Cond, y=price, fill=Overall.Cond)) +
  geom_boxplot(alpha=0.4) + theme(legend.position="none")

grid.arrange(b1, b2, ncol=1, nrow=2)

```

The boxplot shows that there is a relatioship price and the overall quality (rates the overall material and finish of the house). Surprisingly the overall condition does not seem to have the same effect in price. 


* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *


```{r fit_model}
#FREQUENTIST MODEL
Init_model <- lm(log(price) ~ log(area) + log(Lot.Area) + MS.SubClass + 
    Overall.Qual + Heating.QC + Age + House.Style + Neighborhood + Pool.Area +Year.Remod.Add, data = ames_train)

summary(Init_model)

```

It appears that most of the explanatory variables contained within this model are statistically significant predictors of house price, evidenced by their low p-values and the 0.9032 Adjusted R-squared.

* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

First a backwards variable selection is applied to the full frequentist model using both AIC and BIC criterion.

```{r model_select}
#AIC MODEL
AIC_model <- stepAIC(Init_model, k=2, trace = FALSE)
summary(AIC_model)

```




```{r}
#BIC MODEL
n <- nrow(ames_train)

BIC_model <- stepAIC(Init_model, k= log(n), trace = FALSE)
summary(BIC_model)

```

The AIC and BIC methods agree with a good part of the variables. This grants more credibility to these variables if they can survive two separate criteria. However; the AIC model, despite of having more variables, shows a higher Adjusted R-squared of 0.9033. 

* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *


```{r model_resid}
par(mfrow=c(2,2))
plot(AIC_model)

```

The residual variance appears to be relatively stable across the fitted values and does not exhibit a pattern. Furthermore, the Residuals vs Fitted plot shows that the residuals roughly follow a normal distribution. It also identifies 3 high leverage outliers (rows 325, 272 and 611). 


* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *



```{r model_rmse}
# Extract Predictions for AIC
AIC_train_predict <- exp(predict(AIC_model, ames_train))

# Extract Residuals
AIC_train_resid <- ames_train$price - AIC_train_predict

# Calculate RMSE
AIC_train_rmse<- sqrt(mean(AIC_train_resid^2, na.rm = TRUE))
AIC_train_rmse
```

```{r}
# Extract Predictions for BIC
BIC_train_predict <- exp(predict(BIC_model, ames_train))

# Extract Residuals
BIC_train_resid <- ames_train$price - BIC_train_predict

# Calculate RMSE
BIC_train_rmse<- sqrt(mean(BIC_train_resid^2, na.rm = TRUE))
BIC_train_rmse
```

The RMSE is calculated in dollars for our model. Since the price was log transformed, we use exponential function to return the prediction to dollar values before calculating the residuals and then calculate the RMSE of the model. In general, a lower RMSD is better than a higher one. Comparing both models, the RMSE (root mean square error) for the AIC_model (21237.05) is smaller than the one presented by the BIC_model (24022.31)

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

Since the Training data was preprocessed we have to the perform the same transformations on the Test Data. Furthermore some variables in the Training Data have levels that do not existed in the training data, the solution was to eliminate this observations from the Test Data.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")


ames_test <- ames_test %>%
  # Normal sale only
  filter(Sale.Condition == 'Normal') %>%
  # Transforming NAs
  mutate(Alley = if_else(is.na(Alley), 'No Alley Access', as.character(Alley)),
         Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
         Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
         Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
         BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
         BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
         Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
         Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
         Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
         Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
         Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
         Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
         Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
         Misc.Feature = if_else(is.na(Misc.Feature), 'No Misc Features', as.character(Misc.Feature)),
         # changing numerical variables to factor
         MS.SubClass = as.factor(MS.SubClass),
         Overall.Qual = as.factor(Overall.Qual),
         Overall.Cond = as.factor(Overall.Cond),
         # Creating Age Feature
         Age = as.numeric(2018 - Year.Built))

#Eliminate variables not present on Training Data
ames_test <- ames_test %>% 
  filter(House.Style != '2.5Fin') %>%
  filter(Neighborhood != 'Landmrk')

```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *


```{r initmodel_test}
# Extract Predictions for bic
AIC_test_predict <- exp(predict(AIC_model, ames_test))

# Extract Residuals
AIC_test_resid <- ames_test$price - AIC_test_predict

# Calculate RMSE
AIC_test_rmse<- sqrt(mean(AIC_test_resid^2, na.rm = TRUE))
AIC_test_rmse

```

The predictions are slightly more accurate on the training data. This is expected since the model was generated from training data. Overall, the RMSE’s from the test and train data are comparable indicating similar accuracy across both data sets.

RMSE Training = 21237.05 (AIC_model)
RMSE Test = 22753.67

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

All of the variables from my previous model will be carried over into this final model since it performed well on out-of-sample data and exhibits a high R-squared.  The model was chosen based on the following criteria:

 1) Size = variables related to area and size: area, Lot.Area, and Garage.Area
 2) Type of construction = consumer have different perceptions of value for different kinds of houses: MS.SubClass, House Style, and Building Type
 3) Quality and condition = variables related to the perception quality and condition of the house: Overall Quality, External Quality, Age, and Remodel date
 4) Confort = variables that inspire sensations of confort: Heating, Central.Air, and Pool.Area


```{r model_playground}
Final_model <- lm(log(price) ~ log(area) + log(Lot.Area) + MS.SubClass + 
    Overall.Qual + Heating.QC + Age + House.Style + Neighborhood + Pool.Area +Year.Remod.Add + Heating + Central.Air + Garage.Area + Exter.Qual + Bldg.Type, data = ames_train)

summary(Final_model)
```

```{r}
#Stepwise selection using BIC criterion (output NOT SHOWN)
finalBIC_model <- step(Final_model, data=ames_train, direction="backward", k=log(length(ames_train)), trace = FALSE)
summary(finalBIC_model)

#Stepwise selection using AIC criterion (output shown)
finalAIC_model <- step(Final_model, data=ames_train, direction="backward", k=2, trace = FALSE)
summary(finalAIC_model)

```

Model selection entails backwards elimination using both BIC and AIC criteria. For the second time the AIC had a better performance than its counterpart.  Only one variable was dropped using AIC criteria: Pool Area.

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *

I performed logarithmic transformations on 3 variables included in my full model: one was the price variable and the other 2 were related the areas. My EDA discovered that these numerical variables exhibit right skew and contain outliers with exceptionally large prices and areas. Log transformations work to reduce the effect of these outliers.


* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *
Instead of dealing with the Year.Built as a categorical variable, it was found easier to correlate between the price and a new numerical variable called Age which would compute the age of the house today base on the Year.Built. Since both variable would be representing the same phenomenon, we would not use the variable Year.Built in our model selection. 

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *
First it was used some intuition and a basic knowledge about the real state market. After that BIC and AIC criterion were employed to select variables from this full model. This involved a process of stepwise, backwards variable elimination. In the end, both criteria produced the same final model.

AIC and BIC criteria were decided upon because they work towards more parsimonious models by applying a penalty term to the number of predictors included. Model strength and complexity contend with one another in an effort to produce strong, yet simple models. These models usually have less variables and assumptions compared to models selected with the sole intent of maximizing predictive power.

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

The initial model predictions performed well on the test data evidenced by comparable RMSE’s; therefore, this prior model could not be considered overfitted. 

Based on those results, I decided to include all of the variables from my previous model in my final model. Similarly, I felt it important to include in the second iteration overall house condition  since overall house quality performed well as a predictor in the first regression. 

The final model also perfomed very well based on the AIC and BIC stepwise functions, since only one feature was dropped from the original . When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model. 

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

```{r}
par(mfrow=c(2,2))
plot(finalAIC_model)
```

The residual variance appears to be relatively stable across the fitted values and does not exhibit a pattern (seen in the second quadrant). The normal Q-Q plot (in the second quadrant) shows that the residuals approximate a normal distribution. There is not much trouble with outliers - even observation 611 exhibits a cooks distance well-below the 1.0 threshold. 

* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *


```{r}
# Extract Predictions for bic
resid.freq <- ames_train$price - exp(finalAIC_model$fitted.values)

rmse.final <- sqrt(mean(resid.freq^2, na.rm = TRUE))
rmse.final
```

The residuals from the final frequentist model exhibit a  RMSE of USD 20,439.40. Which is an improvement compared to the USD 21,237.05 calculated in the initial model.

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

#### Strengths:

* Good adjusted R^2. 
* Does not appear to violate assumptions of linear regression.
* Shows a good RMSE in the test data.

#### Weaknesses:

* The RMSE is over USD 20k 
* The model follows mostly a frequentist approach and thus does not use any priors that could improve its predictive power based on previous knowledge.
* Some of the predictor variables exhibit may multicollinearity, such as the ones related to the area or quality.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

* * *
Like the previous other datasets, we have to perform the transformations:
```{r validation transform}
ames_validation <- ames_validation %>%
  # Normal sale only
  filter(Sale.Condition == 'Normal') %>%
  # Transforming NAs
  mutate(Alley = if_else(is.na(Alley), 'No Alley Access', as.character(Alley)),
         Bsmt.Qual = if_else(is.na(Bsmt.Qual), 'No Basement', as.character(Bsmt.Qual)),
         Bsmt.Cond = if_else(is.na(Bsmt.Cond), 'No Basement', as.character(Bsmt.Cond)),
         Bsmt.Exposure = if_else(is.na(Bsmt.Exposure), 'No Basement', as.character(Bsmt.Cond)),
         BsmtFin.Type.1 = if_else(is.na(BsmtFin.Type.1), 'No Basement', as.character(BsmtFin.Type.1)),
         BsmtFin.Type.2 = if_else(is.na(BsmtFin.Type.2), 'No Basement', as.character(BsmtFin.Type.2)),
         Fireplace.Qu = if_else(is.na(Fireplace.Qu), 'No Fireplace', as.character(Fireplace.Qu)),
         Garage.Type = if_else(is.na(Garage.Type), 'No Garage', as.character(Garage.Type)),
         Garage.Finish = if_else(is.na(Garage.Finish), 'No Garage', as.character(Garage.Finish)),
         Garage.Qual = if_else(is.na(Garage.Qual), 'No Garage', as.character(Garage.Qual)),
         Garage.Cond = if_else(is.na(Garage.Cond), 'No Garage', as.character(Garage.Cond)),
         Pool.QC = if_else(is.na(Pool.QC), 'No Pool', as.character(Pool.QC)),
         Fence = if_else(is.na(Fence), 'No Fence', as.character(Fence)),
         Misc.Feature = if_else(is.na(Misc.Feature), 'No Misc Features', as.character(Misc.Feature)),
         # changing numerical variables to factor
         MS.SubClass = as.factor(MS.SubClass),
         Overall.Qual = as.factor(Overall.Qual),
         Overall.Cond = as.factor(Overall.Cond),
         # Creating Age Feature
         Age = as.numeric(2018 - Year.Built))

#Eliminate variables not present on Training Data
ames_validation <- ames_validation %>% 
  filter(House.Style != '2.5Fin') %>%
  filter(Neighborhood != 'Landmrk') %>%
  filter(MS.SubClass != 150) %>%
  filter(Heating != 'Floor')
  

```

Now we will chack the RMSE of the model applied to the validadtion dataset:

```{r}
pred_validation <- exp(predict(finalAIC_model,ames_validation))
residuals_validation <- ames_validation$price - pred_validation

rmse_validation <- sqrt(mean(residuals_validation^2))
print(list( "Final Model RMSE" = rmse.final, "Validate RMSE" = rmse_validation))

```


When applying the final model to the out-of-sample validation data, the result is a RMSE of USD 21,146.20. The RMSE from the training and validation data are comparable showing that the final model exhibits similar accuracy over both data sets. This lends more credibility to the final model.

```{r}
# Predict prices using final model on new validation data
predict.final <- exp(predict(finalAIC_model, ames_validation, interval = "prediction",level=0.95))

# Calculate proportion of observations that fall within prediction intervals
coverage.prob <- mean(ames_validation$price > predict.final[,"lwr"] &
                            ames_validation$price < predict.final[,"upr"])
coverage.prob
```

The coverage probability of this final model is approximately 95%, thus this model properly reflects uncertainty.

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

In this project, we applied the Frequentist regression methodologies on housing data from Ames, Iowa in order to produce highly predictive house price models with an Ajusted R-squared of  91,3%. 

**FINAL MODEL (finalAIC_model)**

>>lm(formula = log(price) ~ log(area) + log(Lot.Area) + MS.SubClass + 
    Overall.Qual + Heating.QC + Age + House.Style + Neighborhood + 
    Year.Remod.Add + Heating + Central.Air + Garage.Area + Exter.Qual, 
    data = ames_train)
    
**Residual standard error**: 0.1125 on 761 degrees of freedom

**Multiple R-squared:**  0.9207	

**Adjusted R-squared:**  0.9132 

**F-statistic:** 122.8 on 72 and 761 DF  

**p-value:** < 2.2e-16

**Model RMSE:** 20439.4


The project offers a perfect sense of closure for the Specialization, since it was applied a different variety of techniques and its rationale. The exploratory data analysis provided opportunity to search for variables that might benefit from transformations, and also to create preliminary hypotheses regarding variable correlation with house price. 

Despite of using the frequetist approach, the use of Bayesian Approach  would also be interesting and would have the advantage of fitting priors that could quantify better some prior knowledge regarding the dataset.

The exercise also suggest that this type of analysis offers a lot of opportunity for profit in the real state segment. Since most houses with a market price lower than what the model predicts for that house is considered to be “under-valued”; however, it determines the feasibility of profiting and the profit margin itself.


```{r}
ames_train <- ames_train%>%
        mutate(valuation.freq =ifelse(resid.freq<0, "Under-Valued", "Over-Valued"))

ggplot(ames_train, aes(x=exp(finalAIC_model$fitted.values), y=price, color = valuation.freq)) +
  geom_point(alpha = .3) +
  geom_smooth(method='lm',formula=y~x, se=FALSE, colour="blue", size=.5) +
  theme(legend.title=element_blank()) +
  labs(title="Model Validadtion", x="Fitted Prices", y= "Observed Prices") +
  scale_color_manual(values = c("Under-Valued" = "red",'Over-Valued' = 'orange'))
```



* * *
